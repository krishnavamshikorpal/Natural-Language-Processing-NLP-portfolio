{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc21ea8f",
   "metadata": {},
   "source": [
    "It is better to perform lower case the text as the first step in this text preprocessing. Because if we are trying to remove stop words all words need to be in lower case.\n",
    "\n",
    "For example, few sentences have the starting word as \"The\" if we are not performing the lower casing technique before that technique, we can not remove all stopwords. \n",
    "\n",
    "The other case is for calculating the frequency count. If we not converted the text into lower case Data Science and data science will treat as different tokens.\n",
    "\n",
    "In natural language processing the lower dimension of text which is words called as tokens.\n",
    "\n",
    "We can apply this method to most of the text related problems. Still, it may not be suitable for different projects like Parts-Of-Speech tag recognition or dependency parsing, where proper word casing is essential to recognize nouns, verbs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of lower case conversion\n",
    "\n",
    "def lower_case_convertion(text):\n",
    "\t\"\"\"\n",
    "\tInput :- string\n",
    "\tOutput :- lowercase string\n",
    "\t\"\"\"\n",
    "\tlower_text = text.lower()\n",
    "\treturn lower_text\n",
    "\n",
    "\n",
    "ex_lowercase = \"This is an example Sentence for LOWER case conversion\"\n",
    "lowercase_result = lower_case_convertion(ex_lowercase)\n",
    "print(lowercase_result)\n",
    "\n",
    "## Output:: this is an example sentence for lower case conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e81323",
   "metadata": {},
   "source": [
    "This is the second essential preprocessing technique. The chances to get HTML tags in our text data is quite common when we are extracting or scraping data from different websites. \n",
    "\n",
    "We don't get any valuable information from these HTML tags. So it is better to remove them from our text data. We can remove these tags by using regex and we can also use the BeautifulSoup module from bs4 libraries. \n",
    "\n",
    "Let us see the implementation using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML tags removal Implementation using regex module\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without Html tags\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\thtml_pattern = r'<.*?>'\n",
    "\twithout_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
    "\treturn without_html\n",
    "\n",
    "ex_htmltags = \"\"\" <body>\n",
    "<div>\n",
    "<h1>Hi, this is an example text with Html tags. </h1>\n",
    "</div>\n",
    "</body>\n",
    "\"\"\"\n",
    "htmltags_result = remove_html_tags(ex_htmltags)\n",
    "print(f\"Result :- \\n {htmltags_result}\")\n",
    "\n",
    "## Output:: Hi, this is an example text with Html tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Removing HTML tags using bs4 library\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags_beautifulsoup(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without Html tags\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tparser = BeautifulSoup(text, \"html.parser\")\n",
    "\twithout_html = parser.get_text(separator = \" \")\n",
    "\treturn without_html\n",
    "\n",
    "ex_htmltags = \"\"\" <body>\n",
    "<div>\n",
    "<h1>Hi, this is an example text with Html tags. </h1>\n",
    "</div>\n",
    "</body>\n",
    "\"\"\"\n",
    "htmltags_result = remove_html_tags_beautifulsoup(ex_htmltags)\n",
    "print(f\"Result :- \\n {htmltags_result}\")\n",
    "\n",
    "## Output:: Hi, this is an example text with Html tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d19800d",
   "metadata": {},
   "source": [
    "URL is the short-form of Uniform Resource Locator. The URLs within the text refer to the location of another website or anything else.\n",
    "\n",
    "**If we are performing any website backlinks analysis, twitter or Facebook in that case, URLs are an excellent choice to keep in text.**\n",
    "\n",
    "Otherwise, from URLs also we can not get any information. So we can remove it from our text. We can remove URLs from the text by using the python Regex library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c91c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Removing URLs  using python regex\n",
    "\n",
    "import re\n",
    "def remove_urls(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without URLs\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\turl_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\twithout_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "\treturn without_urls\n",
    "\n",
    "# example text which contain URLs in it\n",
    "ex_urls = \"\"\"\n",
    "This is an example text for URLs like http://google.com & https://www.facebook.com/ etc.\n",
    "\"\"\"\n",
    "\n",
    "# calling removing_urls function with example text (ex_urls)\n",
    "urls_result = remove_urls(ex_urls)\n",
    "print(f\"Result after removing URLs from text :- \\n {urls_result}\")\n",
    "\n",
    "\n",
    "## Output:: This is an example text for URLs like   &   etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db859e",
   "metadata": {},
   "source": [
    "Removing Numbers\n",
    "We can remove numbers from the text if our problem statement doesn't require numbers. \n",
    "\n",
    "For example, if we are working on financial related problems like banking or insurance-related sectors. We may get information from numbers.\n",
    "\n",
    "In those cases, we shouldn't remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Removing numbers  using python regex\n",
    "\n",
    "import re\n",
    "def remove_numbers(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String without numbers\n",
    "\tinput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tnumber_pattern = r'\\d+'\n",
    "\twithout_number = re.sub(pattern=number_pattern,\n",
    " repl=\" \", string=text)\n",
    "\treturn without_number\n",
    "\n",
    "# example text which contain numbers in it\n",
    "ex_numbers = \"\"\"\n",
    "This is an example sentence for removing numbers like 1, 5,7, 4 ,77 etc.\n",
    "\"\"\"\n",
    "# calling remove_numbers function with example text (ex_numbers)\n",
    "numbers_result = remove_numbers(ex_numbers)\n",
    "print(f\"Result after removing number from text :- \\n {numbers_result}\")\n",
    "\n",
    "## Output:: This is an example sentence for removing numbers like  ,  , ,   ,  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df695c0",
   "metadata": {},
   "source": [
    "Converting numbers to words\n",
    "If our problem statement need valuable information from numbers in that case, we have to convert numbers to words. Similar problem statements which are discussed at the removing numbers (above section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b54871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert numbers to words\n",
    "def num_to_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have all numbers or integers in the form of words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# splitting text into words with space\n",
    "\tafter_spliting = text.split()\n",
    "\n",
    "\tfor index in range(len(after_spliting)):\n",
    "\t\tif after_spliting[index].isdigit():\n",
    "\t\t\tafter_spliting[index] = num2words(after_spliting[index])\n",
    "\n",
    "    # joining list into string with space\n",
    "\tnumbers_to_words = ' '.join(after_spliting)\n",
    "\treturn numbers_to_words\n",
    "\n",
    "# example text which contain numbers in it\n",
    "ex_numbers = \"\"\"\n",
    "This is an example sentence for converting numbers to words like 1 to one, 5 to five, 74 to seventy-four, etc.\n",
    "\"\"\"\n",
    "# calling remove_numbers function with example text (ex_numbers)\n",
    "numners_result = num_to_words(ex_numbers)\n",
    "print(f\"Result after converting numbers to its words from text :- \\n {numners_result}\")\n",
    "\n",
    "## Output:: This is an example sentence for converting numbers to words like one to one, five to five, seventy-four to seventy-four, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75e1e82",
   "metadata": {},
   "source": [
    "Spelling correction is another important preprocessing technique while working with tweets, comments, etc. Because we can see incorrect spelling words in those areas of text. We need to make those misspelling words to correct spelling words.\n",
    "\n",
    "We can check and replace misspelling words with correct spelling by using two python libraries, one is pyspellchecker, and another one is autocorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementation of spelling correction using python pyspellchecker library\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell_corrector = SpellChecker()\n",
    "\n",
    "# spelling correction using spellchecker\n",
    "def spell_correction(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have correct spelling words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# initialize empty list to save correct spell words\n",
    "\tcorrect_words = []\n",
    "\t# extract spelling incorrect words by using unknown function of spellchecker\n",
    "\tmisSpelled_words = spell_corrector.unknown(text.split())\n",
    "\n",
    "\tfor each_word in text.split():\n",
    "\t\tif each_word in misSpelled_words:\n",
    "\t\t\tright_word = spell_corrector.correction(each_word)\n",
    "\t\t\tcorrect_words.append(right_word)\n",
    "\t\telse:\n",
    "\t\t\tcorrect_words.append(each_word)\n",
    "\n",
    "\t# joining correct_words list into single string\n",
    "\tcorrect_spelling = ' '.join(correct_words)\n",
    "\treturn correct_spelling\n",
    "\n",
    "#example text with mis spelling words\n",
    "ex_misSpell_words = \"\"\"\n",
    "This is an example sentence for spell corecton\n",
    "\"\"\"\n",
    "spell_result = spell_correction(ex_misSpell_words)\n",
    "print(f\"Result after spell checking :- \\n{spell_result}\")\n",
    " \n",
    "## Output:: This is an example sentence for spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915b3cb",
   "metadata": {},
   "source": [
    "### Implementation of spelling correction using python autocorrect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementation of spelling correction using python autocorrect library\n",
    "\n",
    "from autocorrect import Speller\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# spelling correction using spellchecker\n",
    "def spell_autocorrect(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text which have correct spelling words\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\tcorrect_spell_words = []\n",
    "\n",
    "\t# initialize Speller object for english language with 'en'\n",
    "\tspell_corrector = Speller(lang='en')\n",
    "\tfor word in word_tokenize(text):\n",
    "\t\t# correct spell word\n",
    "\t\tcorrect_word = spell_corrector(word)\n",
    "\t\tcorrect_spell_words.append(correct_word)\n",
    "\n",
    "\tcorrect_spelling = ' '.join(correct_spell_words)\n",
    "\treturn correct_spelling\n",
    "\n",
    "# another example text with misSpelling words\n",
    "ex_misSpell_words_1 = \"\"\"\n",
    "This is anoter exapl for spell correction\n",
    "\"\"\"\n",
    "spell_result = spell_autocorrect(ex_misSpell_words_1)\n",
    "print(f\"Result :- \\n{spell_result}\")\n",
    "\n",
    "## Output:: This is another example for spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deed2079",
   "metadata": {},
   "source": [
    "Convert accented characters to ASCII characters\n",
    "This is another common preprocessing technique in NLP. We can observe special characters at the top of the common letter or characters if we press a longtime while typing, for example, r√©sum√©. \n",
    "\n",
    "If we are not removing these types of noise from the text, then the model will consider resume and r√©sum√©; both are two different words.\n",
    "\n",
    "Even if both are the same. We can convert this accented character to ASCII characters by using the unidecode library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of accented text to ASCII converter in python\n",
    "\n",
    "import unidecode\n",
    "\n",
    "def accented_to_ascii(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text after converting accented characters\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\t# apply unidecode function on text to convert\n",
    "\t# accented characters to ASCII values\n",
    "\ttext = unidecode.unidecode(text)\n",
    "\treturn text\n",
    "\n",
    "# example text with accented characters\n",
    "ex_accented = \"\"\"\n",
    "This is an example text with accented characters like d√®√®p l√®arning √°nd c√∂mputer v√≠s√≠√∂n etc.\n",
    "\"\"\"\n",
    "accented_result = accented_to_ascii(ex_accented)\n",
    "print(f\"Result after converting accented characters to their ASCII values \\n{accented_result}\")\n",
    "\n",
    "## Output:: This is an example text with accented characters like deep learning and computer vision etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad1dcc",
   "metadata": {},
   "source": [
    "Converting chat conversion words to normal words\n",
    "This is another essential preprocessing technique if we work with chat conversions, or our problem statement requires chat conversion analysis. We need to handle short-form. As nowadays, people use short-form words in their chatting conversions for their simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Converting chat conversion words to normal words\n",
    "\n",
    "def short_to_original(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- text after converting short_form words to original\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\tnew_text = []\n",
    "\tfor w in text.split():\n",
    "\t\tif w.upper() in chat_words_list:\n",
    "\t\t\tnew_text.append(chat_words_map_dict[w.upper()])\n",
    "\t\telse:\n",
    "\t\t\tnew_text.append(w)\n",
    "\treturn \" \".join(new_text)\n",
    "\n",
    "\n",
    "# example text for chat conversation short-form words\n",
    "ex_chat = \"\"\"\n",
    "omg this is an example text for chat conversation.\n",
    "\"\"\"\n",
    "# open short_form file and then read sentences from text file using read())\n",
    "short_form_list = open('short_forms.txt', 'r')\n",
    "chat_words_str = short_form_list.read()\n",
    "\n",
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "\tif line != \"\":\n",
    "\t\tcw = line.split(\"=\")[0]\n",
    "\t\tcw_expanded = line.split(\"=\")[1]\n",
    "\t\tchat_words_list.append(cw)\n",
    "\t\tchat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "\n",
    "# calling function\n",
    "chat_result = short_to_original(ex_chat)\n",
    "print(f\"Result {chat_result}\")\n",
    "\n",
    "## Output :: Result oh my god this is an example text for chat conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97288f",
   "metadata": {},
   "source": [
    "Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\n",
    "\n",
    "An example of a contraction word.\n",
    "\n",
    "\"don't\" is \"do not\" \n",
    "\"should've\" is \"should have\" \n",
    "Nlp models don't know about these contractions; they will consider \"don't\" and \"do not\" both are two different words.\n",
    "\n",
    "We have to choose this technique if our problem statement is required. Otherwise,  leave it as it is.\n",
    "\n",
    "Implementation of expanding contractions\n",
    "In the code below, we are importing the CONTRACTION_MAP dictionary from the contraction file. And then define expand_contractions function to expand contractions if our input text has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2385da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of expanding contractions\n",
    "\n",
    "from contraction import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(contraction):\n",
    "\t# take matching contraction in the text\n",
    "\tmatch = contraction.group(0)\n",
    "\t# first char from matching contraction (D for Doesn't)\n",
    "\tfirst_char = match[0]\n",
    "\tif contraction_mapping.get(match):\n",
    "\t\texpanded_contraction = contraction_mapping.get(match)\n",
    "\telse:\n",
    "\t\texpanded_contraction = contraction_mapping.get(match.lower())\n",
    "\texpanded_contraction = first_char+expanded_contraction[1:]\n",
    "\n",
    "\treturn expanded_contraction\n",
    "\n",
    "# expending contractions\n",
    "contraction_mapping = CONTRACTION_MAP\n",
    "# take all key values from contraction_mapping\n",
    "contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "\n",
    "# example text with contractions\n",
    "ex_contractions = \"\"\"\n",
    "Sometimes our mind doesn't work properly.\n",
    "\"\"\"\n",
    "# substitute result of function in the text\n",
    "expanded_text = contractions_pattern.sub(expand_contractions, ex_contractions)\n",
    "# replacing apostrophe with empty string (to remove apostrophe)\n",
    "expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "print(f\"Result :- \\n{expanded_text}\")\n",
    "\n",
    "## Output:: Sometimes our mind does not work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e33fc0",
   "metadata": {},
   "source": [
    "The aim of usage of lemmatization is similar to the stemming technique to reduce inflection words to their original or base words. But the lemmatization process is different from the above approach.\n",
    "\n",
    "Lemmatization does not only trim the suffix characters; instead, use lexical knowledge bases to get original words. The result of lemmatization is always a meaningful word, not like stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Stemming using PorterStemming from nltk library\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string after stemming\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\t# word tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\n",
    "\tfor index in range(len(tokens)):\n",
    "\t\t# stem word to each word\n",
    "\t\tstem_word = stemmer.stem(tokens[index])\n",
    "\t\t# update tokens list with stem word\n",
    "\t\ttokens[index] = stem_word\n",
    "\n",
    "\t# join list with space separator as string\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# initialize porter stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# example text for stemming technique\n",
    "ex_stem = \"Programers program with programing languages\"\n",
    "stem_result = porter_stemmer(ex_stem)\n",
    "print(f\"Result after stemming technique :- \\n{stem_result}\")\n",
    "\n",
    "## Output:: program program with program languag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of lemmatization using nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string after stemming\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\t# word tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\n",
    "\tfor index in range(len(tokens)):\n",
    "\t\t# lemma word\n",
    "\t\tlemma_word = lemma.lemmatize(tokens[index])\n",
    "\t\ttokens[index] = lemma_word\n",
    "\n",
    "\treturn ' '.join(tokens)\n",
    "\n",
    "# initialize lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "# example text for lemmatization\n",
    "ex_lemma = \"\"\"\n",
    "Programers program with programing languages\n",
    "\"\"\"\n",
    "lemma_result = lemmatization(ex_lemma)\n",
    "print(f\"Result of lemmatization \\n{lemma_result}\")\n",
    "\n",
    "## Output:: Programers program with programing language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6720d8b8",
   "metadata": {},
   "source": [
    "### Implementation of emoji removing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementation of emoji removing\n",
    "\n",
    "def remove_emojis(text):\n",
    "\t\"\"\"\n",
    "\tResult :- string without any emojis in it\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\temoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\twithout_emoji = emoji_pattern.sub(r'',text)\n",
    "\treturn without_emoji\n",
    "\n",
    "\n",
    "# example text for emoji removing technique\n",
    "ex_emoji = \"\"\"\n",
    "This is a test üòª üëçüèø\n",
    "\"\"\"\n",
    "# calling function\n",
    "emoji_result = remove_emojis(ex_emoji)\n",
    "print(f\"Result text after removing emojis :- \\n{emoji_result}\")\n",
    "\n",
    "## Output :: This is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bb0d8",
   "metadata": {},
   "source": [
    "implementation of removing of emoticons\n",
    "To remove emotions from the text, we need a list of emoticons; in this GitHub Repo, we can find all emoticons as a dictionary.\n",
    "\n",
    "We take an EMOTICONS dictionary from that GitHub repo and save it in our system as emoticons_list.py. After that, import that file into our preprocessing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of removing of emoticons\n",
    "\n",
    "from emoticons_list import EMOTICONS\n",
    "def remove_emoticons(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing emoticons\n",
    "\tInput :- string\n",
    "\tOutput :- string\n",
    "\t\"\"\"\n",
    "\temoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "\n",
    "\twithout_emoticons = emoticon_pattern.sub(r'',text)\n",
    "\treturn without_emoticons\n",
    "\n",
    "# example sentence for removing emoticons\n",
    "ex_emoticons = \"\"\"\n",
    "Hello this is a sentence with these 2 emoticons :-) & :-)\n",
    "\"\"\"\n",
    "emoticons_result = remove_emoticons(ex_emoticons)\n",
    "print(f\"After removing emoticons :- \\n{emoticons_result}\")\n",
    "\n",
    "## Ouput:: Hello this is a sentence with these 2 emoticons  &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71bc1f",
   "metadata": {},
   "source": [
    "Converting Emojis to words\n",
    "\n",
    "In the previous section, we removed emojis from the text, but some problem statements get information from emojis.\n",
    "\n",
    "In that case, we shouldn't remove emojis.\n",
    "\n",
    "For example, if we are working on sentiment analysis on restaurant reviews data. One review is\n",
    "\n",
    "\"i ordered fried rice that is, üòã üòã\"\n",
    "another review is\n",
    "\n",
    "\"i ordered fried rice that is üòûüò†\".\n",
    "If we remove emojis from these two sentences. We cannot get the user's sentiment. So, in this case, we can convert emojis into words. \n",
    "\n",
    "Implementation of converting emoji to words using python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of converting emoji to words using python\n",
    "\n",
    "from emoticons_list import EMO_UNICODE\n",
    "\n",
    "def emoji_words(text):\n",
    "\tfor emot in UNICODE_EMO:\n",
    "\t\temoji_pattern = r'('+emot+')'\n",
    "\t\t# replace\n",
    "\t\temoji_words = UNICODE_EMO[emot]\n",
    "\t\treplace_text = emoji_words.replace(\",\",\"\")\n",
    "\t\treplace_text = replace_text.replace(\":\",\"\")\n",
    "\t\treplace_text_list = replace_text.split()\n",
    "\t\temoji_name = '_'.join(replace_text_list)\n",
    "\t\ttext = re.sub(emoji_pattern, emoji_name, text)\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# convert emo_unicode to unicode_emo\n",
    "UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}\n",
    "# example text for converting emojis to words\n",
    "ex_emoji = \"\"\"\n",
    "This is a test üòª üëçüèø\n",
    "\"\"\"\n",
    "\n",
    "emoji_result = emoji_words(ex_emoji)\n",
    "print(f\"Result after converting emojis to corresponding words :- \\n{emoji_result}\")\n",
    "\n",
    "## Output:: This is a test smiling_cat_face_with_heart-eyes thumbs_updark_skin_tone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581add2",
   "metadata": {},
   "source": [
    "Converting Emoticons to words\n",
    "The purpose of converting emoticons to words is also the same as converting emojis to words techniques. The only difference is here, converting emoticons to words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of converting emoticons to words\n",
    "\n",
    "from emoticons_list import EMOTICONS\n",
    "\n",
    "def emoticons_words(text):\n",
    "\tfor emot in EMOTICONS:\n",
    "\t\temoticon_pattern = r'('+emot+')'\n",
    "\t\t# replace\n",
    "\t\temoticon_words = EMOTICONS[emot]\n",
    "\t\treplace_text = emoticon_words.replace(\",\",\"\")\n",
    "\t\treplace_text = replace_text.replace(\":\",\"\")\n",
    "\t\treplace_text_list = replace_text.split()\n",
    "\t\temoticon_name = '_'.join(replace_text_list)\n",
    "\t\ttext = re.sub(emoticon_pattern, emoticon_name, text)\n",
    "\treturn text\n",
    "\n",
    "\n",
    "# example sentence for converting  emoticons to words\n",
    "ex_emoticons = \"\"\"\n",
    "Hello this is a sentence with these 2 emoticons :-) & :-)\n",
    "\"\"\"\n",
    "emoticons_result = emoticons_words(ex_emoticons)\n",
    "print(f\"After converting emoticons to words :- \\n{emoticons_result}\")\n",
    "\n",
    "## Output:: Hello this is a sentence with these 2 emoticons Happy_face_smiley & Happy_face_smiley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14440cab",
   "metadata": {},
   "source": [
    "Removing of Punctuations or Special Characters\n",
    "\n",
    "Punctuations or special characters are all characters except digits and alphabets. List of all available special characters are [!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~].  \n",
    "\n",
    "This is better to remove or convert emoticons before removing punctuations or special characters.\n",
    "\n",
    "If we apply this technique process before emoticons related techniques, we may lose emoticons from the text. So if we apply the emoticons technique, apply before removing the punctuation technique.\n",
    "\n",
    "For example, if we remove the period using the punctuation removing technique from text like \"money 20.98\", we will lose the period (.) between 20 & 98. That completely lost their meaning.\n",
    "\n",
    "So we have to focus more on choosing punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7703df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of removing punctuations using string library\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing punctuations\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\treturn text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "\n",
    "# example text for removing punctuations\n",
    "ex_punct = \"\"\"\n",
    "this is an example text for punctuations like .?/*\n",
    "\"\"\"\n",
    "punct_result = remove_punctuation(ex_punct)\n",
    "print(f\"Result after removing punctuations :- \\n{punct_result}\")\n",
    "\n",
    "## Output:: this is an example text for punctuations like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47271f6",
   "metadata": {},
   "source": [
    "Removing of Stopwords\n",
    "\n",
    "Stopwords are common words and irrelevant words from which we can't get any useful information for our model or problem statement.\n",
    "\n",
    "Few stopwords are \"a\", \"an\", \"the\", etc.  \n",
    "\n",
    "For example, we can ignore stop words when we work with sentiment analysis, text classification problems. But in the case of POS (Parts-Of-Speech) tagging or language translation, we have to consider whether stop words also give more information and useful words for our problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5dd1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementation of removing stopwords using all stop words from nltk, spacy, gensim\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing stopwords\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\ttext_without_sw = []\n",
    "\t# tokenization\n",
    "\ttext_tokens = word_tokenize(text)\n",
    "\tfor word in text_tokens:\n",
    "\t\t# checking word is stopword or not\n",
    "\t\tif word not in all_stopwords:\n",
    "\t\t\ttext_without_sw.append(word)\n",
    "\n",
    "\t# joining all tokens after removing stop words\n",
    "\twithout_sw = ' '.join(text_without_sw)\n",
    "\treturn without_sw\n",
    "\n",
    "\n",
    "# list of stopwords from nltk\n",
    "stopwords_nltk = list(stopwords.words('english'))\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "# list of stopwords from spacy\n",
    "stopwords_spacy = list(sp.Defaults.stop_words)\n",
    "# list of stopwords from gensim\n",
    "stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "\n",
    "# unique stopwords from all stopwords\n",
    "all_stopwords = []\n",
    "all_stopwords.extend(stopwords_nltk)\n",
    "all_stopwords.extend(stopwords_spacy)\n",
    "all_stopwords.extend(stopwords_gensim)\n",
    "# all unique stop words\n",
    "all_stopwords = list(set(all_stopwords))\n",
    "print(f\"Total number of Stopwords :- {len(all_stopwords)}\")\n",
    "\n",
    "# example text for stop words removing\n",
    "ex_sw = \"\"\"\n",
    "this is an example text for stopwords such as a, an, the etc.\n",
    "\"\"\"\n",
    "sw_result = remove_stopwords(ex_sw)\n",
    "\n",
    "print(f\"Result after removing stopwords :- \\n{sw_result}\")\n",
    "\n",
    "## Output:: example text stopwords , , ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c0418",
   "metadata": {},
   "source": [
    "Removing of Frequent words\n",
    "In the above section, we removed stopwords.\n",
    "\n",
    "Stopwords are common words all over the language. These frequent words are common words of a particular domain.\n",
    "\n",
    "If we are working on any problem statement for a specific field, we can ignore common words in that domain because those frequent words don't give too much information.\n",
    "\n",
    "Implementation of frequent words removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10389499",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of frequent words removing\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def freq_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- Most frequent words\n",
    "\tInput :- string\n",
    "\tOutput :-\n",
    "\t\"\"\"\n",
    "\t# tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tcounter[word]= +1\n",
    "\n",
    "\tFrequentWords = []\n",
    "\t# take top 10 frequent words\n",
    "\tfor (word, word_count) in counter.most_common(10):\n",
    "\t\tFrequentWords.append(word)\n",
    "\n",
    "\treturn FrequentWords\n",
    "\n",
    "def remove_fw(text, FrequentWords):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing frequent words\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\n",
    "\ttokens = word_tokenize(text)\n",
    "\twithout_fw = []\n",
    "\tfor word in tokens:\n",
    "\t\tif word not in FrequentWords:\n",
    "\t\t\twithout_fw.append(word)\n",
    "\n",
    "\twithout_fw = ' '.join(without_fw)\n",
    "\treturn without_fw\n",
    "\n",
    "\n",
    "# initiate object for counter\n",
    "counter = Counter()\n",
    "# some random text on machine learning\n",
    "ex_fw = \"\"\"\n",
    "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\n",
    "For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It's the same algorithm but it's fed different training data so it comes up with different classification logic.\n",
    "Two kinds of Machine Learning Algorithms\n",
    "You can think of machine learning algorithms as falling into one of two main categories -- supervised learning and unsupervised learning. The difference is simple, but really important.\n",
    "Supervised Learning\n",
    "Let's say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there's a problem -- you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don't have your experience so they don't know how to price their houses.\n",
    "To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it's size, neighborhood, etc, and what similar houses have sold for.\n",
    "So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details -- number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\n",
    "This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\n",
    "To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\n",
    "This kind of like having the answer key to a math test with all the arithmetic symbols erased:\n",
    "\"\"\"\n",
    "\n",
    "# calling count_fw to calculate frequent words\n",
    "FrequentWords = freq_words(ex_fw)\n",
    "print(f\"Top 10 Frequent Words from our example text :- \\n{FrequentWords}\")\n",
    "\n",
    "\n",
    "# calling remove_fw to remove frequent words from example text\n",
    "fw_result = remove_fw(ex_fw, FrequentWords)\n",
    "\n",
    "print(f\"Result after removing frequent words :-\\n{fw_result}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Output::\n",
    "\n",
    "can tell you something interesting about a set of data without you having to write any custom code specific to problem . Instead of writing code , you feed data to algorithm and it builds its own logic based on data . For example , one kind of algorithm a classification algorithm . It can put data into different groups . The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code . It ' s same algorithm but it ' s fed different training data so it comes up with different classification logic . Two kinds of Learning Algorithms You can think of machine as falling into one of two main categories -- supervised and unsupervised . The difference simple , but really important . Supervised Learning Let ' s say you a real estate agent . Your business growing , so you hire a bunch of new trainee agents to help you out . But ' s a problem -- you can glance at a house and have a pretty good of what a house worth , but your trainees don ' t have your experience so they don ' t know how to price their houses . To help your trainees ( and maybe free yourself up for a vacation ) , you decide to write a little app can estimate value of a house in your area based on it ' s size , neighborhood , etc , and what similar houses have sold for . So you write down every time someone sells a house in your city for 3 months . For each house , you write down a bunch of details -- number of bedrooms , size in square feet , neighborhood , etc . But most importantly , you write down final sale price : This called supervised . You knew how much each house sold for , so in other words , you knew answer to problem and could work backwards from to figure out logic . To build your app , you feed your training data about each house into your machine algorithm . The algorithm trying to figure out what kind of math needs to be done to make numbers work out . This kind of like having answer key to a math test with all arithmetic symbols erased\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82d804",
   "metadata": {},
   "source": [
    "Removing of Rare words\n",
    "Removing rare words text preprocessing technique is similar to eliminating frequent words. We can remove more irregular words from the corpus.\n",
    "\n",
    "Implementation of frequent words removing\n",
    "In the below script, the same as the above one, we defined two functions: finding rare words and removing them. We take only ten rare words for this sample text; this number may increase based on our text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269e77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of rare words removing\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def rare_words(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- Most Rare words\n",
    "\tInput :- string\n",
    "\tOutput :- list of rare words\n",
    "\t\"\"\"\n",
    "\t# tokenization\n",
    "\ttokens = word_tokenize(text)\n",
    "\tfor word in tokens:\n",
    "\t\tcounter[word]= +1\n",
    "\n",
    "\tRareWords = []\n",
    "\tnumber_rare_words = 10\n",
    "\t# take top 10 frequent words\n",
    "\tfrequentWords = counter.most_common()\n",
    "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
    "\t\tRareWords.append(word)\n",
    "\n",
    "\treturn RareWords\n",
    "\n",
    "def remove_rw(text, RareWords):\n",
    "\t\"\"\"\n",
    "\tReturn :- String after removing frequent words\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\n",
    "\ttokens = word_tokenize(text)\n",
    "\twithout_rw = []\n",
    "\tfor word in tokens:\n",
    "\t\tif word not in RareWords:\n",
    "\t\t\twithout_rw.append(word)\n",
    "\n",
    "\twithout_rw = ' '.join(without_fw)\n",
    "\treturn without_rw\n",
    "\n",
    "\n",
    "# initiate object for counter\n",
    "counter = Counter()\n",
    "# some random text on machine learning\n",
    "ex_fw = \"\"\"\n",
    "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.\n",
    "For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It's the same algorithm but it's fed different training data so it comes up with different classification logic.\n",
    "Two kinds of Machine Learning Algorithms\n",
    "You can think of machine learning algorithms as falling into one of two main categories -- supervised learning and unsupervised learning. The difference is simple, but really important.\n",
    "Supervised Learning\n",
    "Let's say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there's a problem -- you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don't have your experience so they don't know how to price their houses.\n",
    "To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it's size, neighborhood, etc, and what similar houses have sold for.\n",
    "So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details -- number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:\n",
    "This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.\n",
    "To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.\n",
    "This kind of like having the answer key to a math test with all the arithmetic symbols erased:\n",
    "\"\"\"\n",
    "\n",
    "# calling rare_words to calculate rare words\n",
    "RareWords = rare_words(ex_fw)\n",
    "print(f\"Top 10 Rarer Words from our example text :- \\n{RareWords}\\n\")\n",
    "\n",
    "# calling remove_fw to remove rare words from example text\n",
    "rw_result = remove_fw(ex_fw, RareWords)\n",
    "\n",
    "print(f\"Result after removing rare words :-\\n{rw_result}\")\n",
    "\n",
    "\"\"\"\n",
    "Output::\n",
    "\n",
    "Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem . Instead of writing code , you feed data to the generic algorithm and it builds its own logic based on the data . For example , one kind of algorithm is a classification algorithm . It can put data into different groups . The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code . It ' s the same algorithm but it ' s fed different training data so it comes up with different classification logic . Two kinds of Machine Learning Algorithms You can think of machine learning algorithms as falling into one of two main categories -- supervised learning and unsupervised learning . The difference is simple , but really important . Supervised Learning Let ' s say you are a real estate agent . Your business is growing , so you hire a bunch of new trainee agents to help you out . But there ' s a problem -- you can glance at a house and have a pretty good idea of what a house is worth , but your trainees don ' t have your experience so they don ' t know how to price their houses . To help your trainees ( and maybe free yourself up for a vacation ) , you decide to write a little app that can estimate the value of a house in your area based on it ' s size , neighborhood , etc , and what similar houses have sold for . So you write down every time someone sells a house in your city for 3 months . For each house , you write down a bunch of details -- number of bedrooms , size in square feet , neighborhood , etc . But most importantly , you write down the final sale price : This is called supervised learning . You knew how much each house sold for , so in other words , you knew the answer to the problem and could work backwards from there to figure out the logic . To build your app , you feed your training data about each house into your machine learning algorithm . The algorithm is trying to figure out what kind of math needs to be to the numbers work out . This kind of having the answer to a math with the\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601c86c",
   "metadata": {},
   "source": [
    "Removing single characters\n",
    "After performing all text preprocessing techniques except extra spaces, removing this is better to remove a single character if there is any present in our corpus. We can remove using regex.\n",
    "\n",
    "Implementation of removing single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bbe6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Remove single characters\n",
    "\n",
    "def remove_single_char(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing single characters\n",
    "\tInput :- string\n",
    "\tOutput:- string\n",
    "\t\"\"\"\n",
    "\tsingle_char_pattern = r'\\s+[a-zA-Z]\\s+'\n",
    "\twithout_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "\treturn without_sc\n",
    "\n",
    "# example text for removing single characters\n",
    "ex_sc = \"\"\"\n",
    "this is an example of single characters like a , b , and c .\n",
    "\"\"\"\n",
    "# calling remove_sc function to remove single characters\n",
    "sc_result = remove_single_char(ex_sc)\n",
    "print(f\"Result :-\\n{sc_result}\")\n",
    "\n",
    "## Output:: this is an example of single characters like , , and ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030110e8",
   "metadata": {},
   "source": [
    "Removing Extra Whitespaces\n",
    "This is the last preprocessing technique. We can not get any information from extra spaces, so that we can ignore all additional spaces such as 0ne or more newlines, tabs, extra spaces.\n",
    "\n",
    "Our suggestion is to apply this preprocessing technique at last after performing all text preprocessing techniques.\n",
    "\n",
    "Implementation  of removing extra whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Removing Extra Whitespaces\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "\t\"\"\"\n",
    "\tReturn :- string after removing extra whitespaces\n",
    "\tInput :- String\n",
    "\tOutput :- String\n",
    "\t\"\"\"\n",
    "\tspace_pattern = r'\\s+'\n",
    "\twithout_space = re.sub(pattern=space_pattern, repl=\" \", string=text)\n",
    "\treturn without_space\n",
    "\n",
    "\n",
    "# example text for removing extra spaces\n",
    "ex_space = \"\"\"\n",
    "this      is an\n",
    "extra spaces        .\n",
    "\"\"\"\n",
    "\n",
    "space_result = remove_extra_spaces(ex_space)\n",
    "print(f\"Result :- \\n{space_result}\")\n",
    "\n",
    "## Output:: this is an extra spaces ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d1eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0bb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a3fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38458ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
